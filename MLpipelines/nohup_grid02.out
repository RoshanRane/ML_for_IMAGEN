nohup: ignoring input
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-audit-fu3-audit-freq-audit-quick-n713.h5
time:  2021-06-16 19:30:58.783643
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 57.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
Finished after 0:01:22s with test_score = 61.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
Finished after 0:01:51s with test_score = 45.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 56.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
Finished after 0:01:28s with test_score = 54.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  13.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.0s
Finished after 0:02:11s with test_score = 54.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 52.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 51.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
Finished after 0:02:29s with test_score = 55.23
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 70.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.4s
[CV] END ..................................model_LR__C=0.001; total time=   0.3s
[CV] END ..................................model_LR__C=0.001; total time=   0.3s
Finished after 0:00:04s with test_score = 49.80
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
TOTAL RUNTIME: 0:03:41 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-audit-fu3-audit-total-audit-n789.h5
time:  2021-06-16 19:34:39.825967
TOTAL RUNTIME: 0:04:14 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-audit-gm-fine-cluster-audit-growth-n848.h5
time:  2021-06-16 19:38:54.797893
TOTAL RUNTIME: 0:03:51 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-espad-fu3-19a-binge-n722.h5
time:  2021-06-16 19:42:46.263782
TOTAL RUNTIME: 0:05:17 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-espad-fu3-29d-onset-15-n768.h5
time:  2021-06-16 19:48:03.314504
TOTAL RUNTIME: 0:04:06 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-espad-fu3-8c-frequency-n778.h5
time:  2021-06-16 19:52:09.656638
TOTAL RUNTIME: 0:04:08 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-espad-fu3-prev31-amount-n713.h5
time:  2021-06-16 19:56:18.309345
TOTAL RUNTIME: 0:03:43 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-espad-gm-fine-cluster-binge-growth-n889.h5
time:  2021-06-16 20:00:02.041216
TOTAL RUNTIME: 0:04:37 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-our-combo-cluster-combined-ours-n909.h5
time:  2021-06-16 20:04:39.411467
TOTAL RUNTIME: 0:04:39 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-bl-phenotype-phenotype-combined-seo-n841.h5
time:  2021-06-16 20:09:18.950283
TOTAL RUNTIME: 0:04:08 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-audit-fu3-audit-freq-audit-quick-n727.h5
time:  2021-06-16 20:13:27.720796
TOTAL RUNTIME: 0:03:28 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-audit-fu3-audit-total-audit-n807.h5
time:  2021-06-16 20:16:56.071434
TOTAL RUNTIME: 0:03:49 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-audit-gm-fine-cluster-audit-growth-n802.h5
time:  2021-06-16 20:20:46.059306
TOTAL RUNTIME: 0:04:11 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-espad-fu3-19a-binge-n736.h5
time:  2021-06-16 20:24:57.119689
TOTAL RUNTIME: 0:03:38 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-espad-fu3-29d-onset-15-n780.h5
time:  2021-06-16 20:28:36.048062
TOTAL RUNTIME: 0:03:48 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-espad-fu3-8c-frequency-n804.h5
time:  2021-06-16 20:32:24.835289
TOTAL RUNTIME: 0:03:46 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-espad-fu3-prev31-amount-n734.h5
time:  2021-06-16 20:36:11.438006
TOTAL RUNTIME: 0:03:25 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-espad-gm-fine-cluster-binge-growth-n883.h5
time:  2021-06-16 20:39:36.531422
TOTAL RUNTIME: 0:04:05 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-our-combo-cluster-combined-ours-n861.h5
time:  2021-06-16 20:43:42.503888
TOTAL RUNTIME: 0:04:20 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu2-phenotype-phenotype-combined-seo-n883.h5
time:  2021-06-16 20:48:02.857638
TOTAL RUNTIME: 0:04:32 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-audit-fu3-audit-freq-audit-quick-n722.h5
time:  2021-06-16 20:52:35.299514
TOTAL RUNTIME: 0:03:14 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-audit-fu3-audit-total-audit-n810.h5
time:  2021-06-16 20:55:49.863702
TOTAL RUNTIME: 0:03:43 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-audit-gm-fine-cluster-audit-growth-n678.h5
time:  2021-06-16 20:59:33.642741
TOTAL RUNTIME: 0:03:09 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-espad-fu3-19a-binge-n752.h5
time:  2021-06-16 21:02:43.640377
TOTAL RUNTIME: 0:03:39 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-espad-fu3-29d-onset-15-n811.h5
time:  2021-06-16 21:06:23.075566
TOTAL RUNTIME: 0:04:01 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-espad-fu3-8c-frequency-n831.h5
time:  2021-06-16 21:10:24.156067
TOTAL RUNTIME: 0:04:04 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-espad-fu3-prev31-amount-n744.h5
time:  2021-06-16 21:14:28.783259
TOTAL RUNTIME: 0:03:32 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-espad-gm-fine-cluster-binge-growth-n750.h5
time:  2021-06-16 21:18:00.817610
TOTAL RUNTIME: 0:03:26 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-our-combo-cluster-combined-ours-n848.h5
time:  2021-06-16 21:21:27.110851
TOTAL RUNTIME: 0:04:06 secs
========================================
Running MLpipeline on file:
 /ritter/share/data/IMAGEN/h5files/fulldata-fu3-phenotype-phenotype-combined-seo-n690.h5
time:  2021-06-16 21:25:33.145280
TOTAL RUNTIME: 0:03:01 secs
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 59.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.1s
Finished after 0:02:03s with test_score = 50.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 54.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
Finished after 0:01:42s with test_score = 63.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.3s
Finished after 0:01:47s with test_score = 57.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 49.72
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 45.01
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
Finished after 0:01:34s with test_score = 49.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:51s with test_score = 55.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
Finished after 0:01:49s with test_score = 59.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
Finished after 0:01:49s with test_score = 65.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 54.34
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 48.75
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.4s
Finished after 0:01:56s with test_score = 49.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 50.19
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 42.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
Finished after 0:01:18s with test_score = 51.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
Finished after 0:02:04s with test_score = 46.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 51.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 48.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
Finished after 0:01:43s with test_score = 45.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 51.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 49.81
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
Finished after 0:01:57s with test_score = 56.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  19.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
Finished after 0:01:51s with test_score = 51.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
Finished after 0:00:24s with test_score = 52.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.7s
Finished after 0:00:25s with test_score = 50.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
Finished after 0:01:37s with test_score = 54.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
Finished after 0:01:10s with test_score = 51.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 57.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:19s with test_score = 53.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
Finished after 0:02:06s with test_score = 53.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 51.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 61.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
Finished after 0:01:48s with test_score = 51.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
Finished after 0:01:16s with test_score = 54.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 44.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 53.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.4s
Finished after 0:01:35s with test_score = 57.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
Finished after 0:01:17s with test_score = 51.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:18s with test_score = 47.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 54.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  15.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=  18.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.7s
Finished after 0:02:26s with test_score = 46.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 55.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 61.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
Finished after 0:02:27s with test_score = 64.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.88
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 56.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
Finished after 0:01:44s with test_score = 45.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
Finished after 0:01:23s with test_score = 50.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 53.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:20s with test_score = 63.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:24s with test_score = 46.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:09s with test_score = 50.37
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 49.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 61.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
Finished after 0:01:19s with test_score = 47.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:07s with test_score = 46.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 50.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 49.41
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   3.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:15s with test_score = 48.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
Finished after 0:01:27s with test_score = 50.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:18s with test_score = 52.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 56.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
Finished after 0:01:32s with test_score = 58.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
Finished after 0:01:24s with test_score = 49.29
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 57.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:20s with test_score = 54.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
Finished after 0:01:28s with test_score = 62.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:13s with test_score = 58.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 43.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 62.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
Finished after 0:00:56s with test_score = 59.74
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
Finished after 0:00:57s with test_score = 48.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 61.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 49.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
Finished after 0:02:12s with test_score = 52.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 63.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 52.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
Finished after 0:01:51s with test_score = 56.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
Finished after 0:01:29s with test_score = 56.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
Finished after 0:01:25s with test_score = 44.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 53.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 48.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:20s with test_score = 52.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
Finished after 0:01:19s with test_score = 45.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 55.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 45.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
Finished after 0:01:30s with test_score = 49.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:12s with test_score = 51.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:13s with test_score = 53.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:14s with test_score = 53.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:15s with test_score = 44.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
Finished after 0:01:18s with test_score = 50.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 48.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 62.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
Finished after 0:01:22s with test_score = 46.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.8s
Finished after 0:01:07s with test_score = 54.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 50.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 53.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:30s with test_score = 51.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
Finished after 0:01:15s with test_score = 54.23
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 58.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
Finished after 0:00:12s with test_score = 52.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
Finished after 0:01:19s with test_score = 48.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 46.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:50s with test_score = 52.16
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
Finished after 0:01:22s with test_score = 51.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 53.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 56.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
Finished after 0:02:21s with test_score = 50.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:30s with test_score = 46.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:20s with test_score = 59.41
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
Finished after 0:01:46s with test_score = 58.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 65.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 61.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
Finished after 0:01:12s with test_score = 55.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.3s
Finished after 0:00:58s with test_score = 52.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:09s with test_score = 49.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.5s
Finished after 0:00:45s with test_score = 57.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 48.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 57.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.4s
Finished after 0:02:05s with test_score = 55.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 50.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 48.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
Finished after 0:01:26s with test_score = 52.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  20.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
Finished after 0:01:57s with test_score = 58.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 52.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 57.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
Finished after 0:01:39s with test_score = 51.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.7s
Finished after 0:01:17s with test_score = 49.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:21s with test_score = 57.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:20s with test_score = 49.43
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   3.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  14.6s
Finished after 0:02:12s with test_score = 63.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 58.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.2s
Finished after 0:01:48s with test_score = 52.37
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:31s with test_score = 50.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:20s with test_score = 41.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:17s with test_score = 56.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.8s
Finished after 0:02:00s with test_score = 60.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 51.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 58.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
Finished after 0:01:32s with test_score = 45.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
Finished after 0:01:26s with test_score = 49.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 56.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
Finished after 0:00:19s with test_score = 45.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.7s
Finished after 0:01:45s with test_score = 51.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 62.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.46
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
Finished after 0:01:29s with test_score = 56.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  22.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
Finished after 0:02:13s with test_score = 51.01
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 45.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 55.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:34s with test_score = 61.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.1s
Finished after 0:01:03s with test_score = 62.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 54.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 65.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.9s
Finished after 0:01:43s with test_score = 52.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 47.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 53.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
Finished after 0:02:13s with test_score = 58.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 51.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 41.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
Finished after 0:01:50s with test_score = 51.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 50.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 56.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
Finished after 0:01:48s with test_score = 49.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
Finished after 0:01:23s with test_score = 52.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:20s with test_score = 55.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
Finished after 0:00:22s with test_score = 57.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
Finished after 0:01:22s with test_score = 58.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
Finished after 0:01:24s with test_score = 53.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:22s with test_score = 56.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 60.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
Finished after 0:01:22s with test_score = 55.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
Finished after 0:01:13s with test_score = 57.65
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 67.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 60.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
Finished after 0:00:56s with test_score = 52.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
Finished after 0:00:57s with test_score = 49.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 59.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 48.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
Finished after 0:01:16s with test_score = 49.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
Finished after 0:01:08s with test_score = 54.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
Finished after 0:00:12s with test_score = 43.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:11s with test_score = 49.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
Finished after 0:01:07s with test_score = 64.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
Finished after 0:01:01s with test_score = 50.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 51.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.4s
Finished after 0:00:19s with test_score = 53.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
Finished after 0:01:14s with test_score = 48.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
Finished after 0:01:13s with test_score = 53.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 58.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 51.23
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
Finished after 0:01:25s with test_score = 46.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
Finished after 0:01:21s with test_score = 53.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 48.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 55.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
Finished after 0:01:26s with test_score = 50.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:19s with test_score = 54.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 52.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:12s with test_score = 50.23
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
Finished after 0:01:16s with test_score = 51.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
Finished after 0:01:21s with test_score = 44.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:15s with test_score = 45.80
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:15s with test_score = 60.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
Finished after 0:01:27s with test_score = 56.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 46.99
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 51.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
Finished after 0:02:28s with test_score = 49.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 59.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 53.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
Finished after 0:01:42s with test_score = 58.01
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 56.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
Finished after 0:01:10s with test_score = 56.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
Finished after 0:01:48s with test_score = 50.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 63.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
Finished after 0:01:44s with test_score = 53.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
Finished after 0:01:45s with test_score = 53.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 41.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:20s with test_score = 48.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
Finished after 0:01:30s with test_score = 45.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:51s with test_score = 46.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   2.3s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   2.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   2.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.4s
Finished after 0:00:32s with test_score = 57.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.3s
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.3s
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.3s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.7s
Finished after 0:00:34s with test_score = 73.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 54.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 60.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.7s
Finished after 0:02:18s with test_score = 46.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.0s
Finished after 0:01:07s with test_score = 47.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 52.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 59.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  15.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  13.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.3s
Finished after 0:02:12s with test_score = 44.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 55.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 59.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.1s
Finished after 0:02:11s with test_score = 50.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 55.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 47.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
Finished after 0:02:08s with test_score = 58.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
Finished after 0:01:40s with test_score = 53.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
Finished after 0:00:24s with test_score = 60.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.7s
Finished after 0:00:26s with test_score = 59.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
Finished after 0:01:47s with test_score = 50.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 46.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 54.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
Finished after 0:01:34s with test_score = 52.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
Finished after 0:01:50s with test_score = 48.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 49.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 51.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
Finished after 0:01:31s with test_score = 51.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
Finished after 0:02:25s with test_score = 49.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 50.43
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 63.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
Finished after 0:01:56s with test_score = 51.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  19.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
Finished after 0:01:50s with test_score = 57.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 49.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:19s with test_score = 71.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
Finished after 0:01:42s with test_score = 42.75
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 50.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
Finished after 0:01:17s with test_score = 53.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
Finished after 0:01:35s with test_score = 48.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 48.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 57.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
Finished after 0:01:30s with test_score = 53.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 51.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 61.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
Finished after 0:01:33s with test_score = 53.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
Finished after 0:01:16s with test_score = 44.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 51.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 56.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:22s with test_score = 50.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:20s with test_score = 50.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 49.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 60.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
Finished after 0:01:38s with test_score = 62.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 50.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 56.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
Finished after 0:02:12s with test_score = 55.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
Finished after 0:01:27s with test_score = 59.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 56.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:13s with test_score = 48.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:47s with test_score = 53.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 54.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
Finished after 0:02:28s with test_score = 49.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:06s with test_score = 59.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
Finished after 0:02:13s with test_score = 50.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 59.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 58.74
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
Finished after 0:01:59s with test_score = 68.16
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.1s
Finished after 0:01:01s with test_score = 47.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:20s with test_score = 51.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
Finished after 0:00:18s with test_score = 50.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
Finished after 0:01:48s with test_score = 51.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 48.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
Finished after 0:02:12s with test_score = 47.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.88
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 58.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
Finished after 0:02:20s with test_score = 50.96
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 46.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 49.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
Finished after 0:01:39s with test_score = 55.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
Finished after 0:01:11s with test_score = 53.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 48.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 53.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
Finished after 0:01:25s with test_score = 51.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:17s with test_score = 55.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 50.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
Finished after 0:00:14s with test_score = 57.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
Finished after 0:01:19s with test_score = 58.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 54.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
Finished after 0:01:55s with test_score = 59.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 51.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 52.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.3s
Finished after 0:02:51s with test_score = 52.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 45.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 51.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
Finished after 0:01:36s with test_score = 52.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:49s with test_score = 53.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
Finished after 0:01:55s with test_score = 48.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
Finished after 0:01:47s with test_score = 51.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 51.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:18s with test_score = 57.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.7s
Finished after 0:02:01s with test_score = 52.99
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 42.96
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 46.99
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
Finished after 0:01:28s with test_score = 47.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  20.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
Finished after 0:01:53s with test_score = 53.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 55.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 57.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.3s
Finished after 0:01:50s with test_score = 52.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 63.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
Finished after 0:01:39s with test_score = 51.80
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
Finished after 0:02:04s with test_score = 55.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.3s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
Finished after 0:00:27s with test_score = 55.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   3.6s
Finished after 0:00:35s with test_score = 60.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
Finished after 0:01:45s with test_score = 51.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 69.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
Finished after 0:01:43s with test_score = 54.99
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
Finished after 0:01:43s with test_score = 55.46
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 52.46
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 60.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:28s with test_score = 57.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
Finished after 0:01:12s with test_score = 55.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 49.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 53.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.2s
Finished after 0:01:45s with test_score = 44.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 48.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 53.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
Finished after 0:01:21s with test_score = 52.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  18.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  24.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:02:21s with test_score = 48.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 60.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.8s
Finished after 0:00:21s with test_score = 57.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
Finished after 0:01:40s with test_score = 58.81
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 57.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
Finished after 0:01:39s with test_score = 47.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
Finished after 0:01:25s with test_score = 50.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 51.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 48.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
Finished after 0:01:22s with test_score = 49.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:14s with test_score = 52.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 48.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 49.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.6s
Finished after 0:01:33s with test_score = 48.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 52.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   8.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
Finished after 0:02:53s with test_score = 47.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 58.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 60.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
Finished after 0:02:30s with test_score = 61.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 64.80
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 50.34
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.7s
Finished after 0:02:32s with test_score = 53.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.0s
Finished after 0:01:04s with test_score = 51.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 55.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 47.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
Finished after 0:01:42s with test_score = 61.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 58.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 51.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
Finished after 0:02:22s with test_score = 52.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 47.34
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 49.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
Finished after 0:02:03s with test_score = 50.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
Finished after 0:02:08s with test_score = 59.43
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:06s with test_score = 58.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 52.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
Finished after 0:02:34s with test_score = 51.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.9s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.7s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:06s with test_score = 48.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 60.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
Finished after 0:02:47s with test_score = 54.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 51.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
Finished after 0:02:12s with test_score = 49.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 64.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:01:42s with test_score = 53.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
Finished after 0:01:07s with test_score = 52.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:21s with test_score = 45.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   3.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   3.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   3.3s
Finished after 0:00:32s with test_score = 55.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
Finished after 0:01:27s with test_score = 54.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.3s
Finished after 0:01:05s with test_score = 61.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
Finished after 0:00:12s with test_score = 54.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 52.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:22s with test_score = 50.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 56.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
Finished after 0:01:34s with test_score = 45.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
Finished after 0:01:44s with test_score = 58.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 58.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.9s
Finished after 0:01:56s with test_score = 66.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
Finished after 0:01:50s with test_score = 51.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 49.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
Finished after 0:01:44s with test_score = 52.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:55s with test_score = 60.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   2.2s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   2.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   2.2s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
Finished after 0:00:23s with test_score = 68.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.3s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:30s with test_score = 48.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 49.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 50.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
Finished after 0:01:31s with test_score = 52.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
Finished after 0:01:49s with test_score = 55.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 53.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 47.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
Finished after 0:01:59s with test_score = 50.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
Finished after 0:01:17s with test_score = 50.81
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
Finished after 0:01:46s with test_score = 50.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:25s with test_score = 56.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
Finished after 0:00:21s with test_score = 52.75
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.3s
Finished after 0:01:54s with test_score = 55.75
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.9s
Finished after 0:01:17s with test_score = 41.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 51.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:22s with test_score = 49.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.8s
Finished after 0:01:48s with test_score = 51.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.2s
Finished after 0:01:15s with test_score = 52.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 52.96
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 51.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
Finished after 0:01:26s with test_score = 40.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
Finished after 0:01:24s with test_score = 62.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:15s with test_score = 56.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 61.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
Finished after 0:01:35s with test_score = 47.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 54.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 55.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:01:37s with test_score = 51.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:38s with test_score = 46.62
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 46.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:18s with test_score = 52.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  13.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.2s
Finished after 0:02:09s with test_score = 57.43
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 41.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 59.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
Finished after 0:01:48s with test_score = 70.29
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.6s
Finished after 0:01:15s with test_score = 51.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 49.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 42.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
Finished after 0:01:31s with test_score = 45.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.8s
Finished after 0:01:10s with test_score = 56.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 53.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 55.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
Finished after 0:01:57s with test_score = 49.16
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 62.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
Finished after 0:01:55s with test_score = 49.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
Finished after 0:02:53s with test_score = 54.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 61.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 60.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:01:54s with test_score = 54.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
Finished after 0:01:33s with test_score = 71.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:24s with test_score = 53.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.4s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.1s
Finished after 0:00:25s with test_score = 61.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
Finished after 0:02:06s with test_score = 55.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 42.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 60.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
Finished after 0:02:05s with test_score = 61.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   3.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.1s
Finished after 0:00:37s with test_score = 48.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 58.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:19s with test_score = 49.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
Finished after 0:01:12s with test_score = 49.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
Finished after 0:01:06s with test_score = 49.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:12s with test_score = 59.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:15s with test_score = 52.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:12s with test_score = 56.43
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
Finished after 0:01:02s with test_score = 47.62
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:17s with test_score = 44.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:15s with test_score = 53.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
Finished after 0:01:15s with test_score = 59.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
Finished after 0:01:16s with test_score = 57.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 56.43
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 57.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
Finished after 0:01:31s with test_score = 47.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.37
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 45.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:02:25s with test_score = 51.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 57.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 46.99
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
Finished after 0:02:14s with test_score = 52.46
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 68.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
Finished after 0:02:14s with test_score = 55.88
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 49.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 55.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
Finished after 0:02:28s with test_score = 53.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 49.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 51.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
Finished after 0:01:15s with test_score = 57.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:08s with test_score = 54.44
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 50.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
Finished after 0:01:50s with test_score = 60.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.0s
Finished after 0:01:28s with test_score = 54.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 51.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
Finished after 0:03:06s with test_score = 61.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 54.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:21s with test_score = 48.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
Finished after 0:01:19s with test_score = 51.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:33s with test_score = 51.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
Finished after 0:00:21s with test_score = 46.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.4s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.4s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
Finished after 0:00:25s with test_score = 57.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 36.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
Finished after 0:01:29s with test_score = 50.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
Finished after 0:01:47s with test_score = 51.81
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 41.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 52.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.5s
Finished after 0:02:03s with test_score = 47.01
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
Finished after 0:01:21s with test_score = 49.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
Finished after 0:01:49s with test_score = 45.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   2.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:26s with test_score = 53.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
Finished after 0:00:24s with test_score = 51.41
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.9s
Finished after 0:02:05s with test_score = 53.75
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 47.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 52.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:02:14s with test_score = 54.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
Finished after 0:01:22s with test_score = 60.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 59.13
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
Finished after 0:00:20s with test_score = 65.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
Finished after 0:01:34s with test_score = 53.19
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 54.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 55.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.6s
Finished after 0:02:03s with test_score = 55.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 43.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 54.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
Finished after 0:01:18s with test_score = 48.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
Finished after 0:01:49s with test_score = 42.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 50.46
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:18s with test_score = 49.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.3s
Finished after 0:02:09s with test_score = 48.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 54.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 71.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
Finished after 0:01:18s with test_score = 48.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
Finished after 0:01:34s with test_score = 44.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 48.29
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 50.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
Finished after 0:01:44s with test_score = 54.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 53.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
Finished after 0:01:43s with test_score = 51.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
Finished after 0:01:18s with test_score = 51.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 49.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 61.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:20s with test_score = 51.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
Finished after 0:01:03s with test_score = 55.46
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 52.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:20s with test_score = 60.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
Finished after 0:01:51s with test_score = 52.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 59.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 64.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:02:32s with test_score = 67.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 52.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 48.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
Finished after 0:03:02s with test_score = 58.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 48.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 46.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
Finished after 0:02:06s with test_score = 48.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 56.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 50.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:02:32s with test_score = 56.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 58.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 58.29
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
Finished after 0:02:03s with test_score = 63.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
Finished after 0:02:23s with test_score = 66.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 54.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 68.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
Finished after 0:01:52s with test_score = 53.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
Finished after 0:01:20s with test_score = 48.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 48.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:19s with test_score = 56.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:20s with test_score = 51.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:17s with test_score = 51.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 59.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.0s
Finished after 0:00:18s with test_score = 51.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
Finished after 0:01:21s with test_score = 45.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
Finished after 0:01:13s with test_score = 48.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:15s with test_score = 50.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 58.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
Finished after 0:01:30s with test_score = 65.19
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 37.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.8s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 60.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:02:50s with test_score = 69.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 54.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.81
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
Finished after 0:01:24s with test_score = 48.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:00s with test_score = 50.60
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.19
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 48.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
Finished after 0:01:35s with test_score = 49.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:40s with test_score = 48.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 50.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
Finished after 0:02:18s with test_score = 48.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
Finished after 0:01:34s with test_score = 53.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.3s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.3s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:22s with test_score = 49.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
Finished after 0:03:19s with test_score = 51.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.3s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:23s with test_score = 48.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.3s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.3s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:25s with test_score = 48.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 54.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
Finished after 0:01:34s with test_score = 49.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
Finished after 0:01:42s with test_score = 49.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 47.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:19s with test_score = 51.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.4s
Finished after 0:01:52s with test_score = 48.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.9s
Finished after 0:01:07s with test_score = 50.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 53.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 49.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
Finished after 0:02:09s with test_score = 54.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 59.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  13.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
Finished after 0:02:47s with test_score = 54.80
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 50.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
Finished after 0:02:46s with test_score = 60.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 55.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:02:03s with test_score = 49.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:24s with test_score = 59.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 57.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 57.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
Finished after 0:01:35s with test_score = 52.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 45.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 57.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
Finished after 0:01:47s with test_score = 56.62
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.4s
Finished after 0:01:33s with test_score = 46.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 56.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:21s with test_score = 60.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.0s
Finished after 0:01:57s with test_score = 50.28
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 62.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
Finished after 0:01:19s with test_score = 71.23
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
Finished after 0:01:38s with test_score = 65.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 57.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 56.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.9s
Finished after 0:01:40s with test_score = 49.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 39.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 42.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.6s
Finished after 0:02:16s with test_score = 47.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 48.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 54.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
Finished after 0:02:06s with test_score = 49.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 50.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
Finished after 0:02:38s with test_score = 60.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 58.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
Finished after 0:01:44s with test_score = 51.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
Finished after 0:01:40s with test_score = 49.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.2s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.3s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.1s
Finished after 0:00:26s with test_score = 55.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   2.1s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.4s
Finished after 0:00:25s with test_score = 49.88
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
Finished after 0:02:00s with test_score = 50.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 55.41
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 55.19
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
Finished after 0:02:03s with test_score = 54.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   2.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.0s
Finished after 0:00:40s with test_score = 61.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 54.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
Finished after 0:00:18s with test_score = 47.26
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
Finished after 0:01:25s with test_score = 42.84
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.4s
Finished after 0:01:04s with test_score = 47.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:12s with test_score = 58.12
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:12s with test_score = 64.34
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
Finished after 0:01:24s with test_score = 46.85
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.2s
Finished after 0:00:57s with test_score = 50.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 56.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 67.18
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
Finished after 0:01:36s with test_score = 66.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 58.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 56.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
Finished after 0:01:52s with test_score = 50.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
Finished after 0:01:19s with test_score = 50.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.1s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 49.19
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 42.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
Finished after 0:01:29s with test_score = 50.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
Finished after 0:01:15s with test_score = 46.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:14s with test_score = 55.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:14s with test_score = 50.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.2s
Finished after 0:01:25s with test_score = 58.24
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
Finished after 0:01:15s with test_score = 46.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:15s with test_score = 53.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 50.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
Finished after 0:01:33s with test_score = 50.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 50.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 57.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.5s
Finished after 0:02:49s with test_score = 59.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 50.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 56.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
Finished after 0:01:25s with test_score = 59.34
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
Finished after 0:01:02s with test_score = 58.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 59.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 55.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.1s
Finished after 0:02:31s with test_score = 58.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 57.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 53.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
Finished after 0:03:00s with test_score = 59.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 57.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.1s
Finished after 0:02:23s with test_score = 51.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 49.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 46.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.6s
Finished after 0:01:16s with test_score = 59.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
Finished after 0:01:12s with test_score = 64.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.5s
Finished after 0:01:04s with test_score = 63.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.0s
Finished after 0:00:39s with test_score = 68.74
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:16s with test_score = 46.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 51.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  17.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
Finished after 0:02:21s with test_score = 42.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 49.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 49.29
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  11.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.7s
Finished after 0:02:43s with test_score = 48.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 51.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.01
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:01:45s with test_score = 46.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:32s with test_score = 41.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:23s with test_score = 54.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.1s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.5s
Finished after 0:00:24s with test_score = 66.45
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  13.9s
Finished after 0:02:06s with test_score = 56.62
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 62.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.5s
Finished after 0:02:40s with test_score = 59.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 56.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 53.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
Finished after 0:02:13s with test_score = 54.41
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.50
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 51.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
Finished after 0:01:38s with test_score = 57.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.3s
Finished after 0:01:12s with test_score = 46.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.2s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 49.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 52.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.8s
Finished after 0:02:02s with test_score = 56.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 43.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 54.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
Finished after 0:02:16s with test_score = 55.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  16.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   1.0s
Finished after 0:01:34s with test_score = 50.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 47.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 60.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=  15.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=  13.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.8s
Finished after 0:01:59s with test_score = 63.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 59.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 48.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
Finished after 0:02:34s with test_score = 48.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 56.36
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 52.98
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
Finished after 0:01:45s with test_score = 47.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
Finished after 0:01:17s with test_score = 61.32
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 49.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 50.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.7s
Finished after 0:01:21s with test_score = 45.40
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
Finished after 0:00:57s with test_score = 51.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 51.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:19s with test_score = 54.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
Finished after 0:01:45s with test_score = 49.01
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  12.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   0.5s
Finished after 0:01:00s with test_score = 48.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 52.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:19s with test_score = 55.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=  14.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=  12.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=  11.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.7s
Finished after 0:02:10s with test_score = 56.62
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 54.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 62.70
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  13.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
Finished after 0:03:08s with test_score = 65.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 59.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 50.74
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
Finished after 0:02:19s with test_score = 58.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:18s with test_score = 57.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:16s with test_score = 48.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   1.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   1.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
Finished after 0:01:15s with test_score = 48.42
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
Finished after 0:01:16s with test_score = 47.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:14s with test_score = 56.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.4s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.4s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:13s with test_score = 53.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.0s
Finished after 0:01:15s with test_score = 55.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.8s
Finished after 0:01:02s with test_score = 54.81
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
Finished after 0:00:17s with test_score = 49.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.2s
[CV] END .............................model_SVM-lin__C=0.001; total time=   3.0s
Finished after 0:00:22s with test_score = 53.72
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:33s with test_score = 55.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 44.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 59.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
Finished after 0:02:42s with test_score = 50.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.9s
[CV] END ...................................model_LR__C=1000; total time=   0.6s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 48.41
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 49.65
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
Finished after 0:02:34s with test_score = 55.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:05s with test_score = 54.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ...................................model_LR__C=1000; total time=   0.4s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.5s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 55.99
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
Finished after 0:02:39s with test_score = 47.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 52.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 57.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
Finished after 0:01:41s with test_score = 43.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.2s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.7s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.2s
Finished after 0:01:12s with test_score = 51.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.8s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:18s with test_score = 50.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.8s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:17s with test_score = 63.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.3s
Finished after 0:01:33s with test_score = 56.65
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.5s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.0s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.4s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
Finished after 0:01:16s with test_score = 56.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.8s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
Finished after 0:00:13s with test_score = 59.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.5s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.5s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 52.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.6s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.7s
Finished after 0:01:12s with test_score = 52.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.2s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   3.4s
[CV] END .......................model_GB__learning_rate=0.25; total time=   2.6s
Finished after 0:00:54s with test_score = 59.65
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.4s
Finished after 0:02:29s with test_score = 46.52
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:28s with test_score = 51.02
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
Finished after 0:02:37s with test_score = 47.72
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:22s with test_score = 36.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
Finished after 0:01:43s with test_score = 56.54
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:55s with test_score = 57.09
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
Finished after 0:02:11s with test_score = 56.86
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=  10.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=  13.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   9.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:02:23s with test_score = 52.53
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  13.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  12.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  16.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
Finished after 0:03:22s with test_score = 55.91
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:01:14s with test_score = 52.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  13.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.3s
Finished after 0:02:41s with test_score = 56.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:25s with test_score = 53.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
Finished after 0:01:13s with test_score = 42.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:02:13s with test_score = 52.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
Finished after 0:02:14s with test_score = 56.05
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:33s with test_score = 51.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:01:33s with test_score = 57.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   7.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=  14.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=  12.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:02:35s with test_score = 49.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
Finished after 0:01:36s with test_score = 46.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:01:57s with test_score = 55.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.4s
Finished after 0:01:58s with test_score = 51.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:48s with test_score = 48.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.4s
Finished after 0:02:06s with test_score = 43.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:33s with test_score = 47.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.5s
Finished after 0:01:39s with test_score = 61.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:01:40s with test_score = 52.49
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
Finished after 0:02:29s with test_score = 54.15
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:01:34s with test_score = 62.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
Finished after 0:01:58s with test_score = 51.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   9.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=  10.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
Finished after 0:02:15s with test_score = 55.14
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  13.5s
Finished after 0:03:08s with test_score = 55.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:01:20s with test_score = 55.35
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
Finished after 0:01:46s with test_score = 49.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:01:23s with test_score = 50.92
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.2s
Finished after 0:01:44s with test_score = 52.33
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:51s with test_score = 49.31
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
Finished after 0:01:05s with test_score = 56.83
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:01:50s with test_score = 54.10
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
Finished after 0:01:36s with test_score = 66.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   7.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:02:00s with test_score = 66.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.0s
Finished after 0:02:21s with test_score = 45.22
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:36s with test_score = 49.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.6s
Finished after 0:02:10s with test_score = 49.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:01:48s with test_score = 49.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.4s
Finished after 0:01:33s with test_score = 51.47
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:01:56s with test_score = 53.20
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.8s
Finished after 0:01:14s with test_score = 52.90
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:02:05s with test_score = 66.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
Finished after 0:02:03s with test_score = 53.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
Finished after 0:02:00s with test_score = 57.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
Finished after 0:01:18s with test_score = 46.57
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
Finished after 0:01:35s with test_score = 52.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.3s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 59.62
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.6s
Finished after 0:00:13s with test_score = 52.64
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
Finished after 0:02:13s with test_score = 60.71
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:04s with test_score = 58.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.6s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.6s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.7s
Finished after 0:00:15s with test_score = 48.60
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
Finished after 0:03:31s with test_score = 58.21
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.2s
[CV] END ....................................model_LR__C=100; total time=   0.3s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:03s with test_score = 48.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-lin',
                 SVC(kernel='linear', max_iter=10000, probability=True))]), {'model_SVM-lin__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   1.0s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ..............................model_SVM-lin__C=1000; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   1.0s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=100; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END ...............................model_SVM-lin__C=1.0; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   1.0s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
[CV] END .............................model_SVM-lin__C=0.001; total time=   0.9s
Finished after 0:00:21s with test_score = 52.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.9s
[CV] END .......................model_GB__learning_rate=0.05; total time=  11.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   5.7s
[CV] END .......................model_GB__learning_rate=0.05; total time=   8.1s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   6.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   5.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   4.0s
Finished after 0:01:17s with test_score = 48.79
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_GB',
                 XGBClassifier(base_score=None, booster=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, eval_metric='logloss',
                               gamma=None, gpu_id=None, importance_type='gain',
                               interaction_constraints=None, learning_rate=None,
                               max_delta_step=None, max_depth=5,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, n_estimators=100,
                               n_jobs=None, num_parallel_tree=None,
                               random_state=None, reg_alpha=None,
                               reg_lambda=None, scale_pos_weight=None,
                               subsample=1.0, tree_method=None,
                               validate_parameters=None, verbosity=None))]), {'model_GB__learning_rate': [0.05, 0.25]}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 2 candidates, totalling 10 fits
[CV] END .......................model_GB__learning_rate=0.05; total time=   6.6s
[CV] END .......................model_GB__learning_rate=0.05; total time=   7.1s
[CV] END .......................model_GB__learning_rate=0.05; total time=  10.3s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.8s
[CV] END .......................model_GB__learning_rate=0.05; total time=   9.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.8s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.5s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.9s
[CV] END .......................model_GB__learning_rate=0.25; total time=   8.3s
[CV] END .......................model_GB__learning_rate=0.25; total time=   7.5s
Finished after 0:01:34s with test_score = 42.37
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.2s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 60.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_LR', LogisticRegression(max_iter=1000))]), {'model_LR__C': [1000, 100, 1.0, 0.001]}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ...................................model_LR__C=1000; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=100; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.2s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ....................................model_LR__C=1.0; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
[CV] END ..................................model_LR__C=0.001; total time=   0.1s
Finished after 0:00:02s with test_score = 62.95
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 4}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
Finished after 0:01:25s with test_score = 53.29
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 5}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
Finished after 0:01:53s with test_score = 62.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 6}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:12s with test_score = 66.25
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 7}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:00:37s with test_score = 72.04
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  13.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  13.0s
Finished after 0:02:35s with test_score = 53.58
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:28s with test_score = 49.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.0s
Finished after 0:02:42s with test_score = 53.78
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:01:23s with test_score = 50.96
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.2s
Finished after 0:01:28s with test_score = 55.68
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:02:13s with test_score = 50.59
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=  10.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:02:59s with test_score = 52.07
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:01:35s with test_score = 51.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
Finished after 0:02:27s with test_score = 52.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   9.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:02:06s with test_score = 52.63
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.1s
Finished after 0:02:19s with test_score = 49.08
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:43s with test_score = 53.48
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
Finished after 0:01:22s with test_score = 54.37
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:01:49s with test_score = 54.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.3s
Finished after 0:01:14s with test_score = 57.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   9.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   6.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   6.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:02:33s with test_score = 54.97
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  11.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.4s
Finished after 0:01:59s with test_score = 51.06
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:02:08s with test_score = 52.27
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
Finished after 0:01:30s with test_score = 66.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   7.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:02:06s with test_score = 72.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  10.5s
Finished after 0:02:21s with test_score = 54.94
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:24s with test_score = 47.96
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.3s
Finished after 0:01:47s with test_score = 54.69
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:57s with test_score = 50.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.6s
Finished after 0:01:47s with test_score = 50.65
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:36s with test_score = 50.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   8.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
Finished after 0:02:15s with test_score = 47.55
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:01:48s with test_score = 53.73
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.5s
Finished after 0:02:17s with test_score = 60.80
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   9.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   8.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   8.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:02:01s with test_score = 58.03
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   9.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   9.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  12.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=  14.3s
Finished after 0:03:04s with test_score = 48.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:01:25s with test_score = 49.39
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
Finished after 0:01:38s with test_score = 53.23
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_quick'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:01:33s with test_score = 59.93
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   8.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.4s
Finished after 0:02:00s with test_score = 53.77
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
Finished after 0:01:40s with test_score = 51.11
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
Finished after 0:01:34s with test_score = 54.76
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'AUDIT_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
Finished after 0:01:33s with test_score = 51.87
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.4s
Finished after 0:01:42s with test_score = 48.82
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:01:43s with test_score = 51.44
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.8s
Finished after 0:01:46s with test_score = 51.61
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'onset<15'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   8.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:02:08s with test_score = 49.37
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.5s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.9s
Finished after 0:02:04s with test_score = 48.17
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Frequency'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.5s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   6.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
Finished after 0:01:57s with test_score = 50.00
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.4s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   6.2s
Finished after 0:01:45s with test_score = 56.30
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Amount'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.1s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.4s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.5s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:44s with test_score = 47.56
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
Finished after 0:01:27s with test_score = 54.38
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Binge_growth'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   2.4s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   8.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   4.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.5s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.6s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   4.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   3.2s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.0s
Finished after 0:01:56s with test_score = 64.67
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 2}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.4s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   7.0s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
Finished after 0:01:50s with test_score = 57.66
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_ours'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 3}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.2s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.6s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.3s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.4s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.1s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   5.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.2s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.9s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.0s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.1s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   1.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   1.2s
Finished after 0:02:10s with test_score = 55.51
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 0}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   3.0s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   6.1s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   1.6s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   4.5s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   4.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.6s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.2s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   2.3s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   5.9s
Finished after 0:01:29s with test_score = 54.89
--------------------------------------
Starting a new pipeline with:
 {'io': ('X', 'Combined_seo'), 'pipesgrids': (Pipeline(steps=[('varth', VarianceThreshold()), ('scale', StandardScaler()),
                ('model_SVM-rbf', SVC(probability=True))]), {'model_SVM-rbf__C': [1000, 100, 1.0, 0.001], 'model_SVM-rbf__gamma': ['scale', 'auto']}), 'technique': 'cb', 'trial': 1}
Fitting 5 folds for each of 8 candidates, totalling 40 fits
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   6.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   7.8s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   5.0s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   3.7s
[CV] END ..model_SVM-rbf__C=1000, model_SVM-rbf__gamma=scale; total time=   4.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.7s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   3.6s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.0s
[CV] END ...model_SVM-rbf__C=1000, model_SVM-rbf__gamma=auto; total time=   2.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   3.1s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   2.8s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   1.3s
[CV] END ...model_SVM-rbf__C=100, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ....model_SVM-rbf__C=100, model_SVM-rbf__gamma=auto; total time=   0.6s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ...model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=scale; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END ....model_SVM-rbf__C=1.0, model_SVM-rbf__gamma=auto; total time=   0.7s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.9s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END .model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=scale; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.8s
[CV] END ..model_SVM-rbf__C=0.001, model_SVM-rbf__gamma=auto; total time=   0.9s
Finished after 0:01:29s with test_score = 56.61
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
/ritter/roshan/installations/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
