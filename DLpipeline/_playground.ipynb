{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training on IMAGEN data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary modules and select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies and plotting\n",
    "import os, sys, inspect\n",
    "from glob import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# path\n",
    "from pathlib import Path\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load functions from nitorch\n",
    "sys.path.insert(1,\"/ritter/roshan/workspace/nitorch/\")\n",
    "from nitorch.transforms import  ToTensor, SagittalTranslate, SagittalFlip, IntensityRescale \n",
    "from nitorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from nitorch.trainer import Trainer\n",
    "from nitorch.metrics import binary_balanced_accuracy, sensitivity, specificity\n",
    "from nitorch.utils import count_parameters\n",
    "from nitorch.data import *\n",
    "\n",
    "from CNNpipeline import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 2\n",
    "# check_gpu_status(gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.0'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the directory and file\n",
    "DATA_DIR = \"/ritter/share/data/IMAGEN/h5files/\"\n",
    "h5_file = \"fullbrain-fu3-z2-bingel3u6-n*.h5\"\n",
    "h5_file_holdout = \"fullbrain-fu3-hold-z2-bingel3u6-n*.h5\"\n",
    "\n",
    "data = h5py.File(glob(DATA_DIR+h5_file)[0], 'r')\n",
    "label = data.attrs['labels'][0] #'sex'\n",
    "X_dataset = np.array(data['X'])\n",
    "y_dataset = np.array(data[label]) \n",
    "\n",
    "data_hold = h5py.File(glob(DATA_DIR+h5_file_holdout)[0], 'r')\n",
    "X_test = np.array(data_hold['X'])\n",
    "y_test = np.array(data_hold[label]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size=20% and train_size=80% for total, training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_dataset, y_dataset, \n",
    "    test_size=0.20, stratify=y_dataset,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in h5_file: ['Binge', 'X', 'i', 'sex', 'site']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all data</th>\n",
       "      <td>798</td>\n",
       "      <td>457</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training set</th>\n",
       "      <td>556</td>\n",
       "      <td>318</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set</th>\n",
       "      <td>140</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holdout set</th>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                total  class0  class1\n",
       "all data          798     457     341\n",
       "training set      556     318     238\n",
       "validation set    140      80      60\n",
       "holdout set       102      59      43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states of the label 'Binge': [0.0, 1.0]\n",
      "examples of the label: [0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_all = np.concatenate([y_train, y_val, y_test])\n",
    "print(f'keys in h5_file: {[i for i in data.keys()]}')\n",
    "\n",
    "df_size = pd.DataFrame({\n",
    "    \"all data\":       {\"total\":len(y_all), \"class0\":np.sum(y_all), \"class1\":int(len(y_all)-np.sum(y_all))},\n",
    "    \"training set\":   {\"total\":len(y_train), \"class0\":np.sum(y_train), \"class1\":int(len(y_train)-np.sum(y_train))},\n",
    "    \"validation set\": {\"total\":len(y_val), \"class0\":np.sum(y_val), \"class1\":int(len(y_val)-np.sum(y_val))},\n",
    "    \"holdout set\":    {\"total\":len(y_test), \"class0\":np.sum(y_test), \"class1\":int(len(y_test)-np.sum(y_test))},\n",
    "}, dtype=int).T\n",
    "display(df_size)\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"states of the label '{label}': {list(set(y_all))}\")\n",
    "print(f'examples of the label: {y_val[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define Image Augmentations</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = [SagittalFlip(prob=0.5), SagittalTranslate(dist=(-2, 2)), IntensityRescale(masked=False)]\n",
    "transform = transforms.Compose(augmentations + [ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGEN_data_train = myDataset(X_train, y_train, transform=transform)\n",
    "IMAGEN_data_val = myDataset(X_val, y_val, transform=transform)\n",
    "IMAGEN_data_test = myDataset( X_test, y_test, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Visualization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got numpy.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0194ac5abb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMAGEN_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ritter/roshan/workspace/imagen_ml/DLpipeline/CNNpipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# use soft label? # label = 0.98 if self.y[idx]>=0.5 else 0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;31m#         label = torch.FloatTensor([(self.y[idx]>=0.5)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got numpy.float64)"
     ]
    }
   ],
   "source": [
    "for i in [1,29,77,99]:\n",
    "    sample = IMAGEN_data_train[i]\n",
    "    img = sample[\"image\"].numpy()[0]\n",
    "    lbl = sample[\"label\"].numpy()[0]\n",
    "    if lbl: \n",
    "        print(f\"AAM subject {i}\")\n",
    "    else:\n",
    "        print(f\"Control subject {i}\")\n",
    "#     print(\"img shape =\", img.shape, \"value (max, mean, min)=\", (img.max(), round(img.mean(),2), img.min()))\n",
    "    cut_coords = [43,57,45]\n",
    "\n",
    "    show_brain(img, cut_coords, cmap=\"gray\", draw_cross=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN_3D [n(params) = 664002]\n",
      "FCN_3D(\n",
      "  (convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Dropout3d(p=0.2, inplace=False)\n",
      "      (1): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (5): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Dropout3d(p=0.1, inplace=False)\n",
      "      (1): Conv3d(16, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (2): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Conv3d(24, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Dropout3d(p=0.1, inplace=False)\n",
      "      (1): Conv3d(32, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (2): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Conv3d(80, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (5): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ELU(alpha=1.0)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv3d(128, 72, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): BatchNorm3d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0)\n",
      "      (3): Conv3d(72, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (finalconv): Conv3d(16, 2, kernel_size=[4 5 4], stride=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "net = FCN_3D([16,32,128,16], dropout=[0.2,0.1,0.1], debug_print=True)\n",
    "print(\"{} [n(params) = {}]\\n{}\".format(type(net).__name__, count_parameters(net), net))\n",
    "\n",
    "# move it to GPU\n",
    "# net = net.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_n_times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-18593b8259ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call the model_fit function and print the mean and std of the metrics over all trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m best_metric, trial_metrics, models = train_n_times(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSixtyFourNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGEN_data_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGEN_data_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_n_times' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the model_fit function and print the mean and std of the metrics over all trials\n",
    "CNNpipeline(\n",
    "    network=SixtyFourNet, \n",
    "    train_data=IMAGEN_data_train, \n",
    "    val_data=IMAGEN_data_val,\n",
    "    gpu=gpu,\n",
    "    callbacks=callbacks,\n",
    "    pretrained_model=\"results/pretrained_adni/trial_0_BEST_ITERATION.h5\",\n",
    "    metrics=metrics,\n",
    "    trials=trials,\n",
    "    b=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    retain_metric=retain_metric,\n",
    "    show_train_steps=None,\n",
    "    show_validation_epochs=1,\n",
    "    output_dir=output_dir\n",
    "                            **setting,\n",
    "#                             data=data, data_hold=data_hold, \n",
    "                            X=data[setting['i']], y=data[setting['o']], \n",
    "                            X_test=data_hold[setting['i']], y_test=data_hold[setting['o']],\n",
    "                            gpu=devices[i%len(devices)],\n",
    "                            metrics=[binary_balanced_accuracy, sensitivity, specificity], \n",
    "                            save_model=True, output_dir=SAVE_DIR, \n",
    "                            run_id=i, debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0386, -0.2474,  1.4282,  0.1674,  0.4921],\n",
       "        [-0.3980,  0.5387, -1.0471, -0.2570,  0.7532],\n",
       "        [-0.9329, -0.1831, -0.0054,  1.1812, -0.0952]], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2272, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "\n",
    "train_h5 = h5.File(\"/ritter/share/data/ADNI_HDF5/Splits_Eitel/train_AD_CN_2Yr15T_plus_UniqueScreening_quickprep_(96, 114, 96)_reducedValSize.h5\", 'r')\n",
    "val_h5 = h5.File(\"/ritter/share/data/ADNI_HDF5/Splits_Eitel/val_AD_CN_2Yr15T_plus_UniqueScreening_quickprep_(96, 114, 96)_reducedValSize.h5\", 'r')\n",
    "test_h5 = h5.File(\"/ritter/share/data/ADNI_HDF5/Splits_Eitel/holdout_AD_CN_2Yr15T_plus_UniqueScreening_quickprep_(96, 114, 96)_reducedValSize.h5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'X': (697, 96, 114, 96), 'y': (697,)},\n",
       " <KeysViewHDF5 []>,\n",
       " {'X': (100, 96, 114, 96), 'y': (100,)})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:train_h5[k].shape for k in train_h5}, train_h5.attrs.keys(), {k:val_h5[k].shape for k in val_h5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alzheihmers': (797,), 'X': (797, 96, 114, 96), 'i': (797,)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'X_col_names': (0,), 'confs': (0,), 'labels': (1,)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf1 = h5.File(\"/ritter/share/data/IMAGEN/h5files/fullbrain-adni_2Yr15T_plus_UniqueScreening_quickprep_reducedValSize.h5\", 'w')\n",
    "hf1.create_dataset(\"X\", data=np.concatenate((train_h5['X'], val_h5['X'])))\n",
    "hf1.create_dataset(\"Alzheihmers\", data=np.concatenate((train_h5['y'], val_h5['y'])))\n",
    "hf1.create_dataset(\"i\", data=np.arange(0, len(np.concatenate((train_h5['y'], val_h5['y']))))) # dummy\n",
    "hf1.attrs['labels']= [\"Alzheihmers\"] \n",
    "hf1.attrs['confs']= []  # dummy\n",
    "hf1.attrs['X_col_names']= []  # dummy\n",
    "display({k:hf1[k].shape for k in hf1}, {k:hf1.attrs[k].shape for k in hf1.attrs})\n",
    "hf1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alzheihmers': (172,), 'X': (172, 96, 114, 96), 'i': (172,)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'X_col_names': (0,), 'confs': (0,), 'labels': (1,)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf1 = h5.File(\"/ritter/share/data/IMAGEN/h5files/fullbrain-adni-hold_2Yr15T_plus_UniqueScreening_quickprep_reducedValSize.h5\", 'w')\n",
    "hf1.create_dataset(\"X\", data=test_h5['X'])\n",
    "hf1.create_dataset(\"Alzheihmers\", data=test_h5['y'])\n",
    "hf1.create_dataset(\"i\", data=np.arange(len(np.concatenate((train_h5['y'], val_h5['y']))), len(np.concatenate((train_h5['y'], val_h5['y'])))+len(test_h5['y']))) # dummy\n",
    "hf1.attrs['labels']= [\"Alzheihmers\"]\n",
    "hf1.attrs['confs']= []\n",
    "hf1.attrs['X_col_names']= []\n",
    "display({k:hf1[k].shape for k in hf1}, {k:hf1.attrs[k].shape for k in hf1.attrs})\n",
    "hf1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Binge': (696,),\n",
       "  'X': (696, 96, 114, 96),\n",
       "  'i': (696,),\n",
       "  'sex': (696,),\n",
       "  'site': (696,)},\n",
       " {'X_col_names': (696,), 'confs': (2,), 'labels': (1,)})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myh5 = h5.File(\"/ritter/share/data/IMAGEN/h5files/fullbrain-fu3-z2-bingel3u6-n696.h5\", 'r')\n",
    "{k:myh5[k].shape for k in myh5}, {k:myh5.attrs[k].shape for k in myh5.attrs}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
