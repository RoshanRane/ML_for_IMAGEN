{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from os.path import join \n",
    "import os \n",
    "from scikits.bootstrap import ci\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "sns.set(style=\"whitegrid\", context='paper', color_codes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sorted(glob(f\"results/fullbrain-fu3-*/*/run.csv\"))[-1])  \n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"m_types\"] = df.apply(lambda row:  row[\"m__name\"]+\" \"+\n",
    "                         row[\"m__weights_pretrained\"] +\" \"+\n",
    "                         row[\"m__criterion\"], axis=1)\n",
    "df[\"m_types\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 'val_binary_balanced_accuracy'\n",
    "g = sns.violinplot(x=best_metric, data=df, y=\"m_types\")\n",
    "g.set_xlim([0.,1])\n",
    "g.axvline(0.5, lw=2, ls='--', c='gray', label='chance')\n",
    "g.axvline(0.7, lw=1.5, ls=':', c='grey', label='baseline')\n",
    "g.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "metrics = ['loss', 'binary_balanced_accuracy']\n",
    "\n",
    "for ax, metric  in zip(axes.ravel(), metrics):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        ax.plot(eval(row['train_curve_'+metric]), color='b')\n",
    "        ax.plot(eval(row['val_curve_'+metric]), color='r')\n",
    "    ax.set_title(metric.upper())\n",
    "    ax.legend([\"Train\", \"Val\"])\n",
    "    ax.set_xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(glob(f\"results/rsfmri-fu2-binge-gml3u6-n674/20210505-1253/run.csv\")[-1])  #audit-combos*audit-c*22\n",
    "plot_result(df, x=\"test_score\", no_confs=False, input_type='fMRI (19yrs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(df, x=\"specificity\", no_confs=False, input_type='sMRI (22yrs)')\n",
    "plt.show()\n",
    "plot_result(df, x=\"sensitivity\", no_confs=False, input_type='sMRI (22yrs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(glob(\"results/*AUDIT*AUDIT*BINGE*BINGE*/*/run.csv\")[-1])\n",
    "df[\"runtime\"]= df[\"runtime\"]/60\n",
    "sns.barplot(x=\"model\", y=\"runtime\", hue=\"io\", data=df)\n",
    "plt.ylabel(\"in minutes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine result folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join multiple results folders into one\n",
    "join_results = sorted(glob(\"results/fullbrain-fu3-*/*\"))\n",
    "print(\"Combining results in folders:\")\n",
    "for f in join_results: print(f)\n",
    "save_to = join_results[-1]\n",
    "print(f\"to one folder: \\n{save_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move\n",
    "# combine the csv files and save in 'save_to' dir\n",
    "# if there are unmerged run().csv files then merge them first\n",
    "for f in join_results:\n",
    "    csvs = glob(f+\"/run*.csv\")\n",
    "    if len(csvs)>1 and not os.path.exists(f+\"/run.csv\"): \n",
    "        df = pd.concat([pd.read_csv(csv) for csv in csvs], ignore_index=True)      \n",
    "        # delete the old temp parallel csv files\n",
    "        os.system(f\"rm {f}/run*.csv\")  \n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')] # drop unnamed columns            \n",
    "        df = df.sort_values(['conf_ctrl_tech','i','o','run_id']) # sort\n",
    "        df.to_csv(f+\"/run.csv\", index=False)  \n",
    "        print(\"merged temp-csvs that were present in folder\", f)\n",
    "        \n",
    "pd.concat([pd.read_csv(f+\"/run.csv\") for f in join_results])\n",
    "runs = pd.concat([pd.read_csv(f+\"/run.csv\") for f in join_results])\n",
    "# drop any duplicates\n",
    "# df = df.loc[:,~df.columns.duplicated()]\n",
    "runs.to_csv(join(save_to,'run.csv'), index=False)\n",
    "print(f\"merged run.csv files from {[f for f in join_results if f!=save_to]} to single run.csv in {save_to}\")\n",
    "runs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the rest of the files to 'save_to' dir but replace the run_ids\n",
    "for folder in join_results[:-1]:\n",
    "    n_run_ids = len(glob(join(save_to,'*.log')))\n",
    "    for path in glob(folder+\"/*.*\"):\n",
    "        file = os.path.basename(path)\n",
    "        run_id = (file.split('.')[0].split('_')[0].replace('run', ''))\n",
    "        try: \n",
    "            run_id = int(run_id)\n",
    "        except:\n",
    "            if 'run.csv' not in file:\n",
    "                print(f\"[WARN] could not figure out the run_id for {path}\")\n",
    "            break\n",
    "        \n",
    "        new_run_id = n_run_ids+run_id\n",
    "        new_path = join(save_to, file.replace(str(run_id), str(new_run_id),  1))\n",
    "        print('Saving file \\n{} as \\n{} \\nby replacing run-id from {} to {}'.format(path, new_path, run_id, new_run_id))\n",
    "        print(move(path, new_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in join_results[:-1]:\n",
    "    # delete the old folder\n",
    "    if len(glob(folder+\"/*.*\"))==1 and os.listdir(folder)[0]=='run.csv':\n",
    "        os.system(f\"rm -rf {folder}\")\n",
    "    else:\n",
    "        print(f\"[WARN] could not move all files in {folder}. exiting.. \")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
